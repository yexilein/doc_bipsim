\section {Perspectives}

Because the hybrid method leads to $O(1)$ complexity for drawing reactions, it is not possible to do substantially better on that part of the algorithm. Emphasis should be placed on winning time for updating reactions.

\paragraph{Selection reaction: tau-leaping} The first idea to explore is abandon the exact framework and only update reactions once in a while by introducing time steps for updates. The problem with this method is that a reactant can run out during the time step and still be consumed, leading to negative chemical population. This has to be avoided at all costs. Automatical time step adjustments, termed \textbf{tau-leaping}, have been proposed to resolve this issue~\citep{cao_avoiding_2005}. However, these methods tend to be less efficient if there is constantly a reactant whose concentration is low, which would be the case in a whole-cell simulation scenario. It could be interesting to derive an algorithm that is able to classify reactions into sensitive (if they involve low-concentration reactants) and unsensitive. It would then assess each pool its own time step, with sensitive reacion rates being updated more often.

\paragraph{Propensity updating: factorizing reactions} A situation were the algorithms we have used perform very poorly is when there is a reactant involved in nearly all reactions (hub in the reactant-reaction network). This means virtually every reaction will change the concentration of this reactant, and every reaction propensity will need to be updated, leading to poor performance (because structures underlying algorithms need to be completely rebuilt, as seen previously in the document). As propensities are of multiplicative nature, this is actually not necessary. Say all propensites in the system are of the form $[A]\times ...$, where $[A]$ is the concentration of the ubiquitous reactant. Then we could store $[A]$ on one side, and the remaining part of the propensites elsewhere. Updating would be easier, only $[A]$ would need to be updated. Drawing would only necessitate the second part of the propensity (renormalizing by $[A]$ does not change the drawing probabilities). This works even if not all reactions depend on $A$ by pooling reactions that depend on a same reactant together. Pooling has to be done carefully to ensure the implementation remains efficient and statistically correct.

\paragraph {Implementation: parallel computing} Using parallel computing can enhance performance. For example, in the case where updates are not a problem and propensities are updated constantly, generating random numbers takes up a large amount of time in the simulation (at least 25\%). A first step would be using a node only for the purpose of computing and storing random numbers. Second, if updtating the system is a problem, this is a task that can be naturally parallelized. Concentration of reactants do not change during update, so there is a limited danger of data corruption by assigning updates to different nodes.
